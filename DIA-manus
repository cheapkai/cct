DIA-manuscripts

cluster.py

import numpy as np 
import pickle as pkl 
import cv2
import argparse
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as hcluster
import pandas as pd 


with open('segments.pkl','rb') as f:
    segments=pkl.load(f)

data=[]
for i in range(len(segments)):
	data.append(segments[i][:2])
data2=[]
for i in range(len(segments)):
	data2.append(segments[i][1:2])

data=np.array(data)
data2=np.array(data2)

thresh = 20
clusters = hcluster.fclusterdata(data2, thresh, criterion="distance")
print((clusters))

print(len(data2))
no_clusters=len(set(clusters))
lines=[]


plt.scatter(*np.transpose(data), c=clusters,cmap="rainbow")
#plt.scatter()
plt.axis("equal")
title = "thresh: %f, no. clu: %d" % (thresh, no_clusters)
plt.title(title)
plt.show()

cluster2.py

import numpy as np 
import pickle as pkl 
import cv2
import argparse
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as hcluster
import pandas as pd 


with open('segments.pkl','rb') as f:
    segments=pkl.load(f)

data=[]
for i in range(len(segments)):
	data.append(segments[i][:2])
data2=[]
for i in range(len(segments)):
	data2.append(segments[i][1:2])

data=np.array(data)
data2=np.array(data2)

thresh = 20
clusters = hcluster.fclusterdata(data2, thresh, criterion="distance")
print((clusters))

print(len(data2))
no_clusters=len(set(clusters))
lines=[]


plt.scatter(*np.transpose(data), c=clusters,cmap="rainbow")
#plt.scatter()
plt.axis("equal")
title = "thresh: %f, no. clu: %d" % (thresh, no_clusters)
plt.title(title)
plt.show()


_init_.py

import json
import logging
import os
import sys
from pprint import pprint
import cv2
import numpy as np
from PIL import Image
from sanskrit_data.schema import ullekhanam
import preprocessing
from skimage import measure
import pickle as pkl

logging.basicConfig(
  level=logging.DEBUG,
  format="%(levelname)s: %(asctime)s {%(filename)s:%(lineno)d}: %(message)s "
)

class DisjointRectangles:
  def __init__(self, segments=None):
    if segments is None:
      segments = []
    self.img_targets = []
    self.merge(segments)

  def overlap(self, testseg):
    #testrect = self.to_rect(testseg)
    for i in range(len(self.img_targets)):
      if self.img_targets[i] == testseg:
        return i
    return -1

  def insert(self, newseg):
    #print "Inserting " + str(newseg)
    i = self.overlap(newseg)
    if i >= 0:
      if self.img_targets[i].score >= newseg.score:
        #print "Skipping " + str(newseg) + " < " + str(self.segments[i])
        return False
      else:
        self.remove(i)
        #for r in self.segments:
        #    print "->  " + str(r)
    #print "--> at " + str(len(self.segments))
    self.img_targets.append(newseg)
    return True

  def merge(self, segments):
    merged = [r for r in segments if self.insert(r)]
    return merged

  def get(self, i):
    return self.img_targets[i] if 0 <= i < len(self.img_targets) else None

  def remove(self, i):
    #print "deleting " + str(i) + "(" + str(len(self.segments)) + "): " + str(self.segments[i])
    if 0 <= i < len(self.img_targets):
      del self.img_targets[i]


class DocImage:
  def __init__(self, imgfile = None, workingImgFile = None):
    self.fname = ""
    self.working_img_rgb = None
    self.working_img_gray = None
    self.img_rgb = None
    self.img_gray = None
    self.OutputFile = None
    self.w = 0
    self.h = 0
    self.ww = 0
    self.wh = 0
    self.img_bin = None

    if imgfile:
      #print "DocImage: loading ", imgfile
      self.update_image_file(imgfile)
    if workingImgFile:
      #print "DocImage: loading ", origImgFile
      self.update_working_file(workingImgFile)

  def update_image_file(self, imgfile):
    self.fname = imgfile
    self.img_rgb = cv2.imread(self.fname)
    self.init()

  def update_working_file(self, workingImgFile):
    self.fname = workingImgFile
    self.working_img_rgb = cv2.imread(self.fname)
    if (self.working_img_rgb is None) :

      temp_img = cv2.cvtColor(self.img_rgb, cv2.COLOR_RGB2BGR)
      pil_im = Image.fromarray(temp_img)
      self.working_img_rgb = DocImage.resize(pil_im, (1920, 1080), False)
      self.working_img_rgb = cv2.cvtColor(np.array(pil_im), cv2.COLOR_RGB2BGR)

      cv2.imwrite(workingImgFile, self.working_img_rgb)
    self.OutputFile = self.working_img_rgb
    self.working_img_gray = cv2.cvtColor(self.working_img_rgb, cv2.COLOR_BGR2GRAY)
    self.ww, self.wh = self.working_img_gray.shape[::-1]
    #logging.info("W width = " + str(self.ww) + ", W ht = " + str(self.wh))

  def init(self):
    self.img_gray = cv2.cvtColor(self.img_rgb, cv2.COLOR_BGR2GRAY)
    self.img_bin = preprocessing.binary_img(self.img_gray)
    self.w, self.h = self.img_gray.shape[::-1]
    #logging.info("width = " + str(self.w) + ", ht = " + str(self.h))

  def from_image(self, img_cv):
    self.img_rgb = img_cv
    self.init()

  @classmethod
  def from_path(cls, path):
    from os.path import join
    [base_path, ext] = os.path.splitext(path)
    workingImgPath = join(base_path + "_working.jpg")
    #logging.info("Image path = " + path)
    #logging.info("Working Image path = " + workingImgPath)
    return DocImage(path, workingImgPath)

  @staticmethod
  def resize( img, box, fit):
    """Downsample the image.
    @param img: Image -  an Image-object
    @param box: tuple(x, y) - the bounding box of the result image
    @param fit: boolean - crop the image to fill the box

    @:returns: file-like-object - save the image into the output stream
    """
    #preresize image with factor 2, 4, 8 and fast algorithm
    factor = 1
    while img.size[0]/factor > 2*box[0] and img.size[1]*2/factor > 2*box[1]:
      factor *=2

    if factor > 1:
      img.thumbnail((img.size[0]/factor, img.size[1]/factor), Image.NEAREST)

    #calculate the cropping box and get the cropped part
    if fit:
      x1 = y1 = 0
      x2, y2 = img.size
      wRatio = 1.0 * x2/box[0]
      hRatio = 1.0 * y2/box[1]
      if hRatio > wRatio:
        y1 = int(y2/2-box[1]*wRatio/2)
        y2 = int(y2/2+box[1]*wRatio/2)
      else:
        x1 = int(x2/2-box[0]*hRatio/2)
        x2 = int(x2/2+box[0]*hRatio/2)
      img = img.crop((x1,y1,x2,y2))

    #Resize the image with best quality algorithm ANTI-ALIAS
    img.thumbnail(box, Image.ANTIALIAS)
    return img

    #save it into a file-like object
  #    img.save(out, "JPEG", quality=100)
  #resize

  def save(self, dstfile):
    cv2.imwrite(dstfile, self.img_rgb)

  def find_matches(self, template_img, thres = 0.7, known_segments = None):
    res = cv2.matchTemplate(self.img_bin,  template_img.img_bin, cv2.TM_CCOEFF_NORMED )
    loc = np.where(res >= float(thres))
    def ptToImgTarget(pt):
      return ullekhanam.Rectangle.from_details(x=pt[0], y= pt[1],
                                               w=template_img.w, h=template_img.h,
                                               score = float("{0:.2f}".format(res[pt[1], pt[0]]))
                                               )
    matches = map(ptToImgTarget, zip(*loc[::-1]))

    if known_segments is None:
      known_segments = DisjointRectangles()
    disjoint_matches = known_segments.merge(matches)
    known_segments.img_targets.sort()
    #for r in known_segments.segments:
    #   logging.info(str(r))
    return disjoint_matches

  def snippet(self, r):
    #template_img = self.img_rgb[r.y:(r.y+r.h), r.x:(r.x+r.w)]
    template_img = self.img_rgb[r["y"]:(r["y"]+r["h"]), r["x"]:(r["x"]+r["w"])]
    template = DocImage()
    template.from_image(template_img)
    return template

  def find_recurrence(self, r, thres = 0.7, known_segments = None):
    #logging.info("Searching for recurrence of " + json.dumps(r))

    template = self.snippet(r)

    if known_segments is None:
      known_segments = DisjointRectangles()
    known_segments.insert(ullekhanam.Rectangle.from_details(x=r["x"], y=r["y"], w=r["w"], h=r["h"]))
    return self.find_matches(template, thres, known_segments)

  def self_to_image(self):
    return self.img_rgb


  def find_text_regions(self, show_int=0, pause_int=0):

    if self.working_img_gray is None:
      img = self.img_gray
      totalArea = self.w * self.h
    else:
      img = self.working_img_gray
      totalArea = self.ww * self.wh

    kernel1 = np.ones((2,2),np.uint8)

    def show_img(name, fname):
      if int(show_int) != 0:
        screen_res = 1280.0, 720.0
        scale_width = screen_res[0] / fname.shape[1]
        scale_height = screen_res[1] / fname.shape[0]
        scale = min(scale_width, scale_height)
        window_width = int(fname.shape[1] * scale)
        window_height = int(fname.shape[0] * scale)
        cv2.namedWindow(name, cv2.WINDOW_NORMAL)
        cv2.resizeWindow(name, window_width, window_height)
        cv2.imshow(name, fname)

      if int(pause_int) != 0:
        cv2.waitKey(0)

    def non_max_suppression_fast(boxes, overlapThresh):
      boxes = np.array(boxes)
      if len(boxes) == 0:
        return []
      # if the bounding boxes integers, convert them to floats --
      # this is important since we'll be doing a bunch of divisions
      if boxes.dtype.kind == "i":
        boxes = boxes.astype("float")
      pick = []
      print("Boxes",boxes)
      x1 = boxes[:,0]
      y1 = boxes[:,1]
      x2 = boxes[:,2]
      y2 = boxes[:,3]

      
      area = (x2 - x1 + 1) * (y2 - y1 + 1)
      idxs = np.argsort(y2)
      
      while len(idxs) > 0:
      
        last = len(idxs) - 1
        i = idxs[last]
        pick.append(i)
      
        xx1 = np.maximum(x1[i], x1[idxs[:last]])
        yy1 = np.maximum(y1[i], y1[idxs[:last]])
        xx2 = np.minimum(x2[i], x2[idxs[:last]])
        yy2 = np.minimum(y2[i], y2[idxs[:last]])
     
        w = np.maximum(0, xx2 - xx1 + 1)
        h = np.maximum(0, yy2 - yy1 + 1)
     
        overlap = (w * h) / area[idxs[:last]]
     
        idxs = np.delete(idxs, np.concatenate(([last],
          np.where(overlap > overlapThresh)[0])))
      
      return boxes[pick].astype("int")


    images1=[]
    show_img('Output0',img)
    images1.append(img)
    

    #ret3,th1 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)
    #show_img('otsuThreshold after adaptive', th1)

    #ret,th2 = cv2.threshold(img,ret3-10,255,cv2.THRESH_BINARY)
    #show_img('binary', th2)


    th1 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 25, 10)
    show_img('adaptiveThreshold', th1)
    images1.append(th1)

    #th1 = cv2.dilate(th1, kernel1, iterations=1)
    #show_img('dilation', th1)
    

    blur = cv2.medianBlur(th1,3)
    show_img('median filtering',blur)
    images1.append(blur)

    #blur = cv2.GaussianBlur(blur,(5,5),0)
    #show_img('blur', blur)

    #opening = cv2.morphologyEx(blur, cv2.MORPH_OPEN, kernel1)
    #closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel1)

    #ret3,th1 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)
    #show_img('otsuThreshold after adaptive', th1)

    img = th1

    boxes_temp = np.zeros(img.shape[:2],np.uint8)
    #logging.info("boxes generated")

    binary = 255-img
    dilation=binary

    #erosion = cv2.erode(binary, kernel1, iterations=1)
    #show_img('Erosion', erosion)

    # dilation = cv2.dilate(dilation,kernel1,iterations = 1)
    # show_img('dilationation', dilation)

    #dilation = cv2.GaussianBlur(dilation,(1,3),0)
    #show_img('GaussianBlur',dilation)

    labels = measure.label(dilation, neighbors=8, background=0)
    mask = np.zeros(dilation.shape, dtype="uint8")

    for label in np.unique(labels):
      
      if label == 0:
        continue

      labelMask = np.zeros(dilation.shape, dtype="uint8")
      labelMask[labels == label] = 255
      numPixels = cv2.countNonZero(labelMask)
     
      if numPixels > 5:
        mask = cv2.add(mask, labelMask)

    show_img('mask', mask)
    images1.append(mask)
    cv2.imwrite('ttttttt.jpg',mask)

    if self.working_img_gray is None:
      factorX = float(1.0)
      factorY = float(1.0)
    else:
      factorX = float(self.w) / float(self.ww)
      factorY = float(self.h) / float(self.wh)
    #        logging.info("factorx:"+str(factorX)+"factory:"+str(factorY))

    # Bounds are a guess work, more can be on it.
    lower_bound = totalArea / 8000
    upper_bound = totalArea / 10
    
    ret,thresh = cv2.threshold(mask.copy(),127,255,0)
    immm,contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)
    

    allsegments = []

    ret,thresh = cv2.threshold(mask.copy(),127,255,0)
    immm,contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)

    print("Lower="+str(lower_bound)+" Upper="+str(upper_bound))

    print("Contours 3 Len = "+str(len(contours)))
    print("contours",contours)
    listx=[]
    listy=[]
    for c in contours:
      coordinates = {'x': 0, 'y':0, 'h':0, 'w':0, 'score':float(0.0)}
      x,y,w,h = cv2.boundingRect(c)
      #            logging.info("x:"+str(x)+"y:"+str(y)+"w:"+str(w)+"h"+str(h))
      if (((w*h) <= lower_bound or (w*h) >= upper_bound)) :
        continue
      # cv2.rectangle(boxes_temp,(x,y),(x+w,y+h),(255,0,0),1)
      listy=[x,y,x+w,y+h]
      listx.append(listy)
      listy=[]


    
    boxes_temp1= non_max_suppression_fast(listx,0.3)
    for c in boxes_temp1:
      cv2.rectangle(boxes_temp,(c[0],c[1]),(c[2],c[3]),(255,0,0),2)
    show_img('Boxes_temp 3',boxes_temp)
    images1.append(boxes_temp)    

    return boxes_temp1,images1



  def add_rectangles(self, sel_areas, color = (0, 0, 255), thickness = 2):
    for rect in sel_areas:
      cv2.rectangle(self.img_rgb, (rect[0], rect[1]),
                    (rect[2],rect[3]), color, thickness)


def main(args):
   img = DocImage(args[0])
   rect = { 'x' : int(args[1]),
            'y' : int(args[2]),
            'w' : int(args[3]), 'h' : int(args[4]), 'score' : float(1.0) }
   logging.info("Template rect = " + json.dumps(rect))
   matches = img.find_recurrence(rect, 0.7)
   pprint(matches)
   logging.info("Total", len(matches), "matches found.")

   #logging.info(json.dumps(matches))
   img.add_rectangles(matches)
   img.add_rectangles([rect], (0, 255, 0))

   cv2.namedWindow('Annotated image', cv2.WINDOW_NORMAL)
   cv2.imshow('Annotated image', img.img_rgb)
   cv2.waitKey(0)
   cv2.destroyAllWindows()

   sys.exit(0)

def mainTEST(arg):
  [bpath, filename] = os.path.split(arg)
  [fname, ext] = os.path.splitext(filename)

  image = Image.open(arg).convert('RGB')
  workingFilename = fname+"_working.jpg"
  out = open(workingFilename, "w")
  img = DocImage.resize(image, (1920,1080), False)
  img.save(out, "JPEG", quality=100)
  out.close()

  img = DocImage(arg,fname+"_working.jpg")
  segments, images1 = img.find_text_regions(1, 1)
  print("special:  ",len(segments))

  with open('segments.pkl','wb') as f:
    pkl.dump(segments,f)

  #for seg in segments:
  
  #  first_snippet = img.snippet(seg)
  #  cv2.imshow('First snippet', first_snippet.img_rgb)
  #  cv2.waitKey(0)
    #first_snippet.save(fname + "_snippet1.jpg")
  #first_snippet = img.snippet(segments[5])
  #cv2.imshow('First snippet', first_snippet.img_rgb)
  #cv2.waitKey(0)
  #first_snippet.save(fname + "_snippet1.jpg")
  
  anno_img = DocImage()
  anno_img.from_image(img.OutputFile)
  anno_img.add_rectangles(segments)
  #    img.annotate(img.find_sections(1,1))
  #img.annotate(img.find_segments(1,1))

  screen_res = 1280.0, 720.0
  scale_width = screen_res[0] / anno_img.w
  scale_height = screen_res[1] / anno_img.h
  scale = min(scale_width, scale_height)
  window_width = int(anno_img.w * scale)
  window_height = int(anno_img.h * scale)

  cv2.namedWindow('Final image', cv2.WINDOW_NORMAL)
  cv2.resizeWindow('Final image', window_width, window_height)

  cv2.imshow('Final image', anno_img.img_rgb)
  images1.append(anno_img.img_rgb)
  cv2.waitKey(0)
  cv2.destroyAllWindows()


  height = sum(image.shape[0] for image in images1)
  width = max(image.shape[1] for image in images1)
  output = np.zeros((height,width,3))
  print(output.shape)

  y = 0
  for image in images1:
    if(len(image.shape)==2):
      image = cv2.cvtColor(image,cv2.COLOR_GRAY2RGB)
      h,w,c = image.shape
      output[y:y+h,0:w,0:c] = image
    else:
      h,w,c = image.shape
      output[y:y+h,0:w,0:c] = image

    y += h
  nmm="test_"+fname+".jpg"
  cv2.imwrite(nmm, output)


if __name__ == "__main__":
  #main(sys.argv[1:])
  mainTEST(sys.argv[1])

preprocessing.py

# -*- coding: utf-8 -*-
"""
pre-processing and pattern matching.
This python module can perform the following functions:
1. Binarization - method binary_img(img) performs this function
2. Skew correction - method skew_correction(img) performs this function
Need to introduce machine learning of some sort to make the skew correction
method run faster :(
Or... A simple fix would be to resize the image first, and then apply the skew
correction method! That'll probably take lesser time...
Resizing is yielding better results.
"""

import logging

import cv2
import numpy as np
from scipy.stats import mode

logging.basicConfig(
  level=logging.DEBUG,
  format="%(levelname)s: %(asctime)s {%(filename)s:%(lineno)d}: %(message)s "
)

kernel = np.ones((5, 5), np.uint8)
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))

"""
Method to binarize an image
Input: Grayscale image
Output: Binary image
The nature of the output is such that the text(foreground) has a colour 
value of (255,255,255), and the background has a value of (0,0,0).
"""


def binary_img(img):
  # img_erode = cv2.dilate(img,kernel,iterations = 2)
  blur = cv2.medianBlur(img, 5)

  # mask1 = np.ones(img.shape[:2],np.uint8)
  """Applying histogram equalization"""
  cl1 = clahe.apply(blur)

  circles_mask = cv2.dilate(cl1, kernel, iterations=1)
  circles_mask = (255 - circles_mask)

  thresh = 1
  circles_mask = cv2.threshold(circles_mask, thresh, 255, cv2.THRESH_BINARY)[1]

  edges = cv2.Canny(cl1, 100, 200)

  edges = cv2.bitwise_and(edges, edges, mask=circles_mask)

  dilation = cv2.dilate(edges, kernel, iterations=1)

  display = cv2.bitwise_and(img, img, mask=dilation)

  cl2 = clahe.apply(display)
  cl2 = clahe.apply(cl2)

  ret, th = cv2.threshold(cl2, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
  th = 255 - th

  thg = cv2.adaptiveThreshold(display, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \
                              cv2.THRESH_BINARY, 11, 2)

  # final = cv2.bitwise_and(dilation,dilation,mask=th)

  finalg = cv2.bitwise_and(dilation, dilation, mask=thg)

  finalg = 255 - finalg

  abso = cv2.bitwise_and(dilation, dilation, mask=finalg)

  return abso


"""
Method to resize the image. This is going to help in reducing the number 
of computations, as the size of data will reduce.
"""


def resize(img):
  r = 1000.0 / img.shape[1]
  dim = (1000, int(img.shape[0] * r))
  resized = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)

  # cv2.imshow('resized', resized)
  return resized


"""
Method to correct the skew of an image
Input: Binary image
Output: Skew corrected binary image
The nature of the output is such that the binary image is rotated appropriately
to remove any angular skew.
Find out the right place to insert the resizing method call.
Try to find one bounding rectangle around all the contours
"""


def skew_correction(img):
  areas = []  # stores all the areas of corresponding contours
  dev_areas = []  # stores all the areas of the contours within 1st std deviation in terms of area#stores all the white pixels of the largest contour within 1st std deviation
  all_angles = []
  k = 0

  binary = binary_img(img)
  # binary = resize(binary)
  im2, contours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
  # cnt = contours[0]
  # upper_bound=len(contours)
  height_orig, width_orig = img.shape[:2]
  words = np.zeros(img.shape[:2], np.uint8)

  for c in contours:
    areas.append(cv2.contourArea(c))

  std_dev = np.std(areas)
  for i in areas:
    dev_areas.append(i - std_dev)

  dev_contours = np.zeros(img.shape[:2], np.uint8)

  for i in dev_areas:
    if ((i > (-std_dev)) and (i <= (std_dev))):
      cv2.drawContours(dev_contours, contours, k, (255, 255, 255), -1)
    k += 1

  sobely = cv2.Sobel(dev_contours, cv2.CV_64F, 0, 1, ksize=5)
  abs_sobel64f = np.absolute(sobely)
  sobel_8u = np.uint8(abs_sobel64f)

  # cv2.imshow('Output2',sobel_8u)

  minLineLength = 100
  maxLineGap = 10
  lines = cv2.HoughLinesP(sobel_8u, 1, np.pi / 180, 100, minLineLength, maxLineGap)

  for x1, y1, x2, y2 in lines[0]:
    cv2.line(words, (x1, y1), (x2, y2), (255, 255, 255), 2)
  # cv2.imshow('hough',words)

  height_orig, width_orig = img.shape[:2]
  all_angles = []

  im2, contours, hierarchy = cv2.findContours(words, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
  logging.debug(len(contours))
  contour_count = 0
  for c in contours:
    # max_index = np.argmax(areas)
    # current_contour = np.zeros(img.shape[:2],np.uint8)
    current_contour = np.zeros(img.shape[:2], np.uint8)
    cv2.drawContours(current_contour, contours, contour_count, (255, 255, 255), -1)

    height, width = current_contour.shape[:2]

    # all_white_pixels = []
    current_white_pixels = []

    for i in range(0, height):
      for j in range(0, width):
        if (current_contour.item(i, j) == 255):
          current_white_pixels.append([i, j])

    matrix = np.array(current_white_pixels)

    """Finding covariance matrix"""
    C = np.cov(matrix.T)

    eigenvalues, eigenvectors = np.linalg.eig(C)

    """Finding max eigenvalue"""
    # max_ev = max(eigenvalues)
    """Finding index of max eigenvalue"""
    max_index = eigenvalues.argmax(axis=0)

    """The largest eigen value gives the approximate length of the bounding
        ellipse around the largest word. If we follow the index of the largest 
        eigen value and find the eigen vectors in the column of that index,
        we'll get the x and y coordinates of it's centre."""
    y = eigenvectors[1, max_index]
    x = eigenvectors[0, max_index]

    angle = (np.arctan2(y, x)) * (180 / np.pi)
    all_angles.append(angle)
    contour_count += 1
    logging.debug(contour_count)

    logging.debug(all_angles)
    angle = np.mean(all_angles)
    logging.debug(angle)

  k = 0
  non_zero_angles = []

  for i in all_angles:
    if ((i != 0) and (i != 90.0)):
      non_zero_angles.append(i)

  logging.debug(non_zero_angles)

  rounded_angles = []
  for i in non_zero_angles:
    rounded_angles.append(np.round(i, 0))

  logging.debug(rounded_angles)
  logging.debug("mode is")
  # logging.debug(np.mode(rounded_angles))
  # angle = np.mean(non_zero_angles)
  # angle = np.mode(rounded_angles)

  mode_angle = mode(rounded_angles)[0][0]
  logging.debug(mode_angle)

  precision_angles = []
  for i in non_zero_angles:
    if (np.round(i, 0) == mode_angle):
      precision_angles.append(i)

  logging.debug('precision angles:')
  logging.debug(precision_angles)

  angle = np.mean(precision_angles)
  logging.debug('Finally, the required angle is:')
  logging.debug(angle)

  # M = cv2.getRotationMatrix2D((width/2,height/2),-(90+angle),1)
  M = cv2.getRotationMatrix2D((width / 2, height / 2), -(90 + angle), 1)
  dst = cv2.warpAffine(img, M, (width_orig, height_orig))

  # cv2.imshow('final',dst)
  cv2.imwrite('skewcorrected2.jpg', dst)

  return dst


def preprocess(img):
  return skew_correction(img)

# Does not work with linux:
# cv2.destroyAllWindows()

resize_img.py

import cv2

def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):
    dim = None
    (h, w) = image.shape[:2]
    if width is None and height is None:
        return image
    if width is None:
        r = height / float(h)
        dim = (int(w * r), height)
    else:
        r = width / float(w)
        dim = (width, int(h * r))
    resized = cv2.resize(image, dim, interpolation = inter)
    return resized
image=cv2.imread("BORI-11-page-001_working.jpg")
image = image_resize(image, height = 2000)
cv2.imwrite("BORI.jpg",image)

sft.py

import cv2
import numpy as np

img = cv2.imread('IMG_0256_srimahaganapati_working.jpg')
gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

orb = cv2.ORB_create(edgeThreshold=5,nfeatures=10000, scoreType=cv2.ORB_HARRIS_SCORE,scaleFactor=1.2) 
kp ,des= orb.detectAndCompute(gray,None)

img=cv2.drawKeypoints(gray,kp,None)

cv2.imwrite('sift_keypoints.jpg',img)

temp.py

import cv2

def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):
    dim = None
    (h, w) = image.shape[:2]
    if width is None and height is None:
        return image
    if width is None:
        r = height / float(h)
        dim = (int(w * r), height)
    else:
        r = width / float(w)
        dim = (width, int(h * r))
    resized = cv2.resize(image, dim, interpolation = inter)
    return resized
image=cv2.imread("BORI-11-page-001_working.jpg")
image = image_resize(image, height = 800)
cv2.imwrite("BORI.jpg",image)

tm.py

import argparse
import json
import os
import pickle as pkl 
import cv2
import numpy as np
import colorsys
from sklearn.metrics import jaccard_similarity_score
from skimage.measure import compare_ssim as ssim
import matplotlib.pyplot as plt 

ap = argparse.ArgumentParser()
ap.add_argument("-i", "--page", required=True,
                help="Path to the image")
ap.add_argument("-m", "--mask", required=True,
                help="Path to the mask")

args = vars(ap.parse_args())
imgs=args["page"]
imgs2=args["mask"]
img=cv2.imread(imgs,1)
img2=cv2.imread(imgs2,0)

with open('segments.pkl','rb') as f:
    segments=pkl.load(f)


mxw=-999999999999
mxh=-999999999999

for i in range(len(segments)):
    mxw=max(mxw,segments[i][2]-segments[i][0])
    mxh=max(mxh,segments[i][3]-segments[i][1])

#print(mxw,"   ",mxh)



#print(segments[0])
segments=sorted(segments,key=lambda x:(x[1]))
segments=sorted(segments,key=lambda x:(x[1]*x[0]))

#print(segments[0])

#print(len(segments))
no_segs=len(segments)
#segments=segments[8:]
#parameters
temp_inc=1
matt_inc=1
Bound_width=5
Bound_height=5
scale_low=97
scale_high=103
rotate_low=0
rotate_high=0
count=0
threshold=0.35
iou_thresh=0.4



matt_aug=[]
count1=0
print("precomputing augmented components")
for segment in segments:
    img2=cv2.imread(imgs2,0)
    matt2=img2[segment[1]-matt_inc:segment[3]+matt_inc,segment[0]-matt_inc:segment[2]+matt_inc]
    matt_aug.append([])
    for i in range(scale_low,scale_high+1,1):
            try:
                res = cv2.resize(matt2,None,fx=float(i/100), fy=float(i/100), interpolation = cv2.INTER_CUBIC)
                matt_aug[count1].append(res)
            except:
                continue
    
    for i in range(rotate_low,rotate_high+1,1):
            try:
                M = cv2.getRotationMatrix2D(((cols-1)/2.0,(rows-1)/2.0),i,1)
                dst = cv2.warpAffine(matt2,M,(cols,rows))
                matt_aug[count1].append(dst)
            except:
                continue
    count1=count1+1

def jacc(matt,template):
    res=jaccard_similarity_score(matt,template)
    return res

def skel(img):
    size = np.size(img)
    skel = np.zeros(img.shape,np.uint8)
    ret,img = cv2.threshold(img,127,255,0)
    element = cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3))
    done = False
     
    while( not done):
        eroded = cv2.erode(img,element)
        temp = cv2.dilate(eroded,element)
        temp = cv2.subtract(img,temp)
        skel = cv2.bitwise_or(skel,temp)
        img = eroded.copy()
     
        zeros = size - cv2.countNonZero(img)
        if zeros==size:
            done = True
    return skel

def IOU(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)
    iou = interArea / float(boxAArea + boxBArea - interArea)
    return iou


print("template matching")
#print(matt_aug)
for segment in segments:

    img=cv2.imread(imgs,1)
    img2=cv2.imread(imgs2,0)
    #template=img2[(1-int(temp_inc/100))*segment[1]:(1+int(temp_inc/100))*segment[3],(1-int(temp_inc/100))*segment[0]:(1+int(temp_inc/100))*segment[2]]
    template=img2[segment[1]-temp_inc:segment[3]+temp_inc,segment[0]-temp_inc:segment[2]+temp_inc]
    cv2.imshow("template",template)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
    template=skel(template)
    template=cv2.resize(template,(mxw,mxh))
    
    #val,template = cv2.threshold(template,100,255,cv2.THRESH_BINARY)
    count=0

    for seg in segments:
        #matt=img2[(1-int(matt_inc/100))*seg[1]:(1+int(matt_inc/100))*seg[3],(1-int(matt_inc/100))*seg[0]:(1+int(matt_inc/100))*seg[2]]
        matt=img2[seg[1]-matt_inc:seg[3]+matt_inc,seg[0]-matt_inc:seg[2]+matt_inc]
        matt=cv2.resize(matt,(mxw,mxh))
        for matt1 in matt_aug[count]:

            #cv2.imshow("matt1",matt1)
            #cv2.waitKey(0)
            #cv2.destroyAllWindows()
            #print("1")
            matt1=cv2.resize(matt1,(mxw,mxh))
            #val,matt1 = cv2.threshold(matt1,100,255,cv2.THRESH_BINARY)


            template=cv2.resize(template,(mxw,mxh))
            res = cv2.matchTemplate(matt1,template,cv2.TM_CCOEFF_NORMED)
            segment_=segment.copy()
            seg_=seg.copy()
            segment_[2]-=segment_[0]
            segment_[3]-=segment_[1]
            segment_[0]-=segment_[0]
            segment_[1]-=segment_[1]
            

            seg_[2]-=seg_[0]
            seg_[3]-=seg_[1]
            seg_[0]-=seg_[0]
            seg_[1]-=seg_[1]
            
            iou=IOU(segment_,seg_)
            
            if(max(res.flatten())>=threshold and iou>=iou_thresh) :
                #print("iou : ",iou)
                cv2.rectangle(img, (seg[0],seg[1]),( seg[2] , seg[3]), (0,0,255), 2)


            '''
            res=jaccard_similarity_score(matt1.flatten(),template.flatten())
            #print(res)
            if(res>0.5):
                cv2.rectangle(img, (seg[0],seg[1]),( seg[2] , seg[3]), (0,0,255), 2)
            '''
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.putText(img,"{:.1f}".format(float(res)),(int(seg[0]),int(seg[3])+5), font, 0.3,(255,0,0),1,cv2.LINE_AA)
        count=count+1
    

    cv2.namedWindow("output", cv2.WINDOW_NORMAL)
    cv2.resizeWindow("output", (int(img.shape[1]), int(img.shape[0])))
    cv2.imshow("output",img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

    
tm1.py

import argparse
import json
import os
import pickle as pkl 
import cv2
import numpy as np
import colorsys

ap = argparse.ArgumentParser()
ap.add_argument("-i", "--page", required=True,
                help="Path to the image")
ap.add_argument("-m", "--mask", required=True,
                help="Path to the mask")

args = vars(ap.parse_args())
imgs=args["page"]
imgs2=args["mask"]
img=cv2.imread(imgs,1)
img2=cv2.imread(imgs2,0)

with open('segments.pkl','rb') as f:
    segments=pkl.load(f)

print(segments[0])
segments=sorted(segments,key=lambda x:(x[1]))
segments=sorted(segments,key=lambda x:(x[1]*x[0]))

print(segments[0])

print(len(segments))
no_segs=len(segments)
temp_inc=5
count=0
scores=[]
for segment in segments:

    img=cv2.imread(imgs,1)
    img2=cv2.imread(imgs2,0)
    template=img2[(1-int(temp_inc/100))*segment[1]:(1+int(temp_inc/100))*segment[3],(1-int(temp_inc/100))*segment[0]:(1+int(temp_inc/100))*segment[2]]
    cv2.imshow("template",template)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
    w, h = template.shape[::-1]
    res = cv2.matchTemplate(img2,template,cv2.TM_CCOEFF_NORMED)
    threshold = 0.6
    loc = np.where( res >= threshold) 
    ct=0
    for pt in zip(*loc[::-1]):
        print(ct)
        cv2.rectangle(img, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)
        ct=ct+1

    cv2.namedWindow("output", cv2.WINDOW_NORMAL)
    cv2.resizeWindow("output", (int(img.shape[1]), int(img.shape[0])))
    cv2.imshow("output",img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
    count=count+1
    print(count)

tm2.py

import argparse
import json
import os
import pickle as pkl 
import cv2
import numpy as np
import colorsys

ap = argparse.ArgumentParser()
ap.add_argument("-i", "--page", required=True,
                help="Path to the image")
ap.add_argument("-m", "--mask", required=True,
                help="Path to the mask")

args = vars(ap.parse_args())
imgs=args["page"]
imgs2=args["mask"]
img=cv2.imread(imgs,1)
img2=cv2.imread(imgs,0)

with open('segments.pkl','rb') as f:
    segments=pkl.load(f)

#print(segments[0])
segments=sorted(segments,key=lambda x:(x[1]))
segments=sorted(segments,key=lambda x:(x[1]*x[0]))

#print(segments[0])
orb = cv2.ORB_create(edgeThreshold=5,nfeatures=50, scoreType=cv2.ORB_HARRIS_SCORE) 
bf = cv2.BFMatcher(cv2.NORM_HAMMING,crossCheck=False)
print(len(segments))
no_segs=len(segments)
temp_inc=0
matt_inc=0
count=0
for segment in segments:
    img=cv2.imread(imgs,1)
    img2=cv2.imread(imgs,0)
    #template=img2[(1-int(temp_inc/100))*segment[1]:(1+int(temp_inc/100))*segment[3],(1-int(temp_inc/100))*segment[0]:(1+int(temp_inc/100))*segment[2]]
    template=img2[segment[1]-temp_inc:segment[3]+temp_inc,segment[0]-temp_inc:segment[2]+temp_inc]
    
    w, h = template.shape[::-1]
    kp1 ,des1= orb.detectAndCompute(template,None)
    cv2.imshow("template",template)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
    #t1=cv2.drawKeypoints(template,kp1,None)
    #cv2.imshow("t1",t1)
    #cv2.waitKey(0)
    #cv2.destroyAllWindows()
    if des1 is None :
        continue
    matches=list()
    for seg in segments:
        #matt=img2[(1-int(matt_inc/100))*seg[1]:(1+int(matt_inc/100))*seg[3],(1-int(matt_inc/100))*seg[0]:(1+int(matt_inc/100))*seg[2]]
        matt=img2[seg[1]-matt_inc:seg[3]+matt_inc,seg[0]-matt_inc:seg[2]+matt_inc]
        #cv2.imshow("matt",matt)
        #cv2.waitKey(0)
        #cv2.destroyAllWindows()
        w_m,h_m=matt.shape[::-1]
        kp2 ,des2= orb.detectAndCompute(matt,None)
        #t2=cv2.drawKeypoints(matt,kp2,None)
        #cv2.imshow("t2",t2)
        #cv2.waitKey(0)
        #cv2.destroyAllWindows()
        if des2 is None:
            continue
        matches = bf.knnMatch(np.asarray(des1, np.uint8), np.asarray(des2, np.uint8), k=2)
        #print(matches)

        if(len(matches)>30):
            cv2.rectangle(img, (seg[0],seg[1]), ( seg[2] , seg[3]), (0,0,255), 2)

    cv2.namedWindow("output", cv2.WINDOW_NORMAL)
    cv2.resizeWindow("output", (int(img.shape[1]), int(img.shape[0])))
    cv2.imshow("output",img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
    count=count+1
    print(count)





