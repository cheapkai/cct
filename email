EmailId

CNN.py

import numpy as np
import cPickle
from collections import defaultdict
import re

import sys
import os

os.environ['KERAS_BACKEND']='tensorflow'

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical

from keras.layers import Embedding
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout
from keras.models import Model

max_seq_len = 1500
emd_dim = 100
val_split = 0.2

emails = []
labels = []

classes = []
class_labels = {}
class_ind = 0
print "6"

for author_dir in os.listdir('clean_enron'):
    if author_dir == '.DS_Store':
        continue
    classes.append(author_dir)
    class_labels[author_dir] = class_ind
    class_ind += 1

for author_dir in os.listdir('clean_enron'):
    if author_dir == '.DS_Store':
        continue
    for message_file in os.listdir('./clean_enron/' + author_dir):
        with open('./clean_enron/' + author_dir + '/' + message_file, 'r') as f:
            text = f.read()
            text = text.replace("\n", " ")
            emails.append(text.lower())
            labels.append(class_labels[author_dir])    

tokenizer = Tokenizer()
tokenizer.fit_on_texts(emails)
sequences = tokenizer.texts_to_sequences(emails)

word_index = tokenizer.word_index

data = pad_sequences(sequences, maxlen=max_seq_len)

labels = to_categorical(np.asarray(labels))

indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
nb_validation_samples = int(val_split * data.shape[0])

x_train = data[:-nb_validation_samples]
y_train = labels[:-nb_validation_samples]
x_val = data[-nb_validation_samples:]
y_val = labels[-nb_validation_samples:]

embeddings_index = {}
with open('glove.6B.100d.txt') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

embedding_matrix = np.random.random((len(word_index) + 1, emd_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
        
embedding_layer = Embedding(len(word_index) + 1,
                            emd_dim,
                            weights=[embedding_matrix],
                            input_length=max_seq_len,
                            trainable=True)

convs = []
filter_sizes = [3,4,5]

sequence_input = Input(shape=(max_seq_len,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)

for fsz in filter_sizes:
    l_conv = Conv1D(128, fsz, activation='relu')(embedded_sequences)
    l_pool = MaxPooling1D(5)(l_conv)
    convs.append(l_pool)
    
l_merge = Merge(mode='concat', concat_axis=1)(convs)
l_cov1 = Conv1D(128, 5, activation='relu')(l_merge)
l_pool1 = MaxPooling1D(5)(l_cov1)
l_drop_1 = Dropout(0.2)(l_pool1)
l_cov2 = Conv1D(128, 5, activation='relu')(l_drop_1)
l_pool2 = MaxPooling1D(30)(l_cov2)
l_drop_2 = Dropout(0.2)(l_pool2)
l_flat = Flatten()(l_drop_2)
l_dense = Dense(128, activation='relu')(l_flat)
l_drop_3 = Dropout(0.3)(l_dense)
preds = Dense(10, activation='softmax')(l_drop_3)

model = Model(sequence_input, preds)
model.compile(loss='categorical_crossentropy',
              optimizer='nadam',
              metrics=['acc'])

model.fit(x_train, y_train, validation_data=(x_val, y_val), 
            epochs=30, batch_size=10)

HierLSTM.py

import numpy as np
import cPickle
from collections import defaultdict
import re

import sys
import os

os.environ['KERAS_BACKEND']='tensorflow'

from keras.preprocessing.text import Tokenizer, text_to_word_sequence
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical

from keras.layers import Embedding
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed
from keras.layers.normalization import BatchNormalization
from keras.models import Model

from keras import backend as K
from keras.engine.topology import Layer, InputSpec

max_sen_len = 100
max_sents = 30
emb_dim = 100
val_split = 0.2

lines = []
labels = []
texts = []

classes = []
class_labels = {}
class_ind = 0

for author_dir in os.listdir('clean_enron'):
    if author_dir == '.DS_Store':
        continue
    classes.append(author_dir)
    class_labels[author_dir] = class_ind
    class_ind += 1

for author_dir in os.listdir('clean_enron'):
    if author_dir == '.DS_Store':
        continue
    for message_file in os.listdir('./clean_enron/' + author_dir):
        with open('./clean_enron/' + author_dir + '/' + message_file, 'r') as f:
            text = f.read()
            sentences = text.lower().split('\n')
            lines.append(sentences)  
            text = text.lower().replace("\n", " ")
            labels.append(class_labels[author_dir])
            texts.append(text)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)

data = np.zeros((len(texts), max_sents, max_sen_len), dtype='int32')

for i, sentences in enumerate(lines):
    for j, sent in enumerate(sentences):
        if j< max_sents:
            wordTokens = text_to_word_sequence(sent)
            k = 0
            for _, word in enumerate(wordTokens):
                if k < max_sen_len:
                    data[i, j, k] = tokenizer.word_index[word]
                    k = k + 1                    
                    
word_index = tokenizer.word_index
labels = to_categorical(np.asarray(labels))

indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
nb_validation_samples = int(val_split * data.shape[0])

x_train = data[:-nb_validation_samples]
y_train = labels[:-nb_validation_samples]
x_val = data[-nb_validation_samples:]
y_val = labels[-nb_validation_samples:]

embeddings_index = {}
with open('glove.6B.100d.txt') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

embedding_matrix = np.random.random((len(word_index) + 1, emb_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
        
embedding_layer = Embedding(len(word_index) + 1,
                            emb_dim,
                            weights=[embedding_matrix],
                            input_length=max_sen_len,
                            trainable=True)

sentence_input = Input(shape=(max_sen_len,), dtype='int32')
embedded_sequences = embedding_layer(sentence_input)
l_lstm = Bidirectional(LSTM(100))(embedded_sequences)
sentEncoder = Model(sentence_input, l_lstm)

email_input = Input(shape=(max_sents,max_sen_len), dtype='int32')
email_encoder = TimeDistributed(sentEncoder)(email_input)
l_lstm_sent = Bidirectional(LSTM(100))(email_encoder)
dense_1 = Dense(128, activation="relu")(l_lstm_sent)
drop_1 = Dropout(0.3)(dense_1)
preds = Dense(10, activation='softmax')(drop_1)
model = Model(email_input, preds)

model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['acc'])

model.fit(x_train, y_train, validation_data=(x_val, y_val),
          epochs=40, batch_size=30)

HierLSTM_withStylometry.py

import numpy as np
import cPickle
from collections import defaultdict
import re
import random
import sys
import os

os.environ['KERAS_BACKEND']='tensorflow'

from keras.preprocessing.text import Tokenizer, text_to_word_sequence
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical

from keras.layers import Embedding
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed
from keras.layers.normalization import BatchNormalization
from keras.layers import merge
from keras.layers import concatenate

from keras.models import Model
from keras import backend as K
from keras.engine.topology import Layer, InputSpec
from keras.callbacks import Callback

max_sen_len = 100
max_sents = 30
emb_dim = 100
val_split = 0.2

lines = []
labels = []
texts = []
f_names = []
style_vectors = []
scd = []
style_dict = {}
classes = []
class_labels = {}
class_ind = 0
style_train = []
style_val = []

# Input the stylometric features 
with open("./extracted_features/stylometricVector.txt", "r") as sv:
    for line in sv:
        scd = line.split(',')
        s_key = scd[0] + "/" + scd[1]

        s_value = map(float, scd[2:])

        style_dict[s_key] = s_value

# Input the class labels
for author_dir in os.listdir('clean_enron'):
    if author_dir == '.DS_Store':
        continue
    classes.append(author_dir)
    class_labels[author_dir] = class_ind
    class_ind += 1

# Input the contents of each email
for author_dir in os.listdir('clean_enron'):
    if author_dir == '.DS_Store':
        continue
    for message_file in os.listdir('./clean_enron/' + author_dir):
        with open('./clean_enron/' + author_dir + '/' + message_file, 'r') as f:
            text = f.read()
            sentences = text.lower().split('\n')
            lines.append(sentences)  
            text = text.lower().replace("\n", " ")
            labels.append(class_labels[author_dir])
            texts.append(text)
            f_names.append(author_dir + '/' + message_file)
            if author_dir + '/' + message_file not in style_dict :
                style_vectors.append([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ])
            else :
                style_vectors.append(style_dict[ author_dir + '/' + message_file ])
            
# Convert stylometric feature vector to numpy array
style_vectors = np.array(style_vectors)

# Normalize the numerical feature vectors
style_vectors = style_vectors / style_vectors.max(axis=0)

#Tokenization
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
data = np.zeros((len(texts), max_sents, max_sen_len), dtype='int32')
for i, sentences in enumerate(lines):
    for j, sent in enumerate(sentences):
        if j< max_sents:
            wordTokens = text_to_word_sequence(sent)
            k = 0
            for _, word in enumerate(wordTokens):
                if k < max_sen_len:
                    data[i, j, k] = tokenizer.word_index[word]
                    k = k + 1                    
word_index = tokenizer.word_index
labels = to_categorical(np.asarray(labels))

# Randomly shuffle the data
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
style_vectors = style_vectors[indices]

# Split the data as training  and validation
nb_validation_samples = int(val_split * data.shape[0])
x_train = data[:-nb_validation_samples]
y_train = labels[:-nb_validation_samples]
x_val = data[-nb_validation_samples:]
y_val = labels[-nb_validation_samples:]
style_train = style_vectors[:-nb_validation_samples]
style_val = style_vectors[-nb_validation_samples:]

## Performance metrics

# F1 score
f1_scores = []

# Precision
precisions = []

# Recall
recalls = []

# Create embeddings
embeddings_index = {}
with open('glove.6B.100d.txt') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

embedding_matrix = np.random.random((len(word_index) + 1, emb_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
        
# Embedding layer
embedding_layer = Embedding(len(word_index) + 1,
                            emb_dim,
                            weights=[embedding_matrix],
                            input_length=max_sen_len,
                            trainable=True)

# Bidirectional LSTM
sentence_input = Input(shape=(max_sen_len,), dtype='int32')
embedded_sequences = embedding_layer(sentence_input)
l_lstm = Bidirectional(LSTM(100))(embedded_sequences)

# Bidirectional LSTM
sentEncoder = Model(sentence_input, l_lstm)
email_input = Input(shape=(max_sents,max_sen_len), dtype='int32')
email_encoder = TimeDistributed(sentEncoder)(email_input)
l_lstm_sent = Bidirectional(LSTM(100))(email_encoder)

# Input the stylometric feature
auxiliary_input = Input(shape=(8,))

# Concatenate with lstm output
x_new = concatenate([l_lstm_sent, auxiliary_input])

# Dense layer
dense_1 = Dense(128, activation='relu')(x_new)

# Dropout layer
drop_1 = Dropout(0.3)(dense_1)

# Final Dense layer
preds = Dense(10, activation='softmax')(drop_1)

# Model
model = Model(inputs=[email_input, auxiliary_input], outputs=preds)

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])

# Summarize the model
model.summary()

# Fit the model
model.fit(x=[x_train, style_train], y=y_train, validation_data=([x_val, style_val], y_val), epochs=30, batch_size=50)

LSTM_final.py

import numpy as np
import cPickle
from collections import defaultdict
import re

import sys
import os

os.environ['KERAS_BACKEND']='tensorflow'

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical

from keras.layers import Embedding
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, GRU, LSTM, Bidirectional
from keras.models import Model

max_sen_len = 1500
emb_dim = 100
val_split = 0.2

emails = []
labels = []

classes = []
class_labels = {}
class_ind = 0

for author_dir in os.listdir('clean_enron'):
    if author_dir == '.DS_Store':
        continue
    classes.append(author_dir)
    class_labels[author_dir] = class_ind
    class_ind += 1

for author_dir in os.listdir('clean_enron'):
    if author_dir == '.DS_Store':
        continue
    for message_file in os.listdir('./clean_enron/' + author_dir):
        with open('./clean_enron/' + author_dir + '/' + message_file, 'r') as f:
            text = f.read()
            text = text.replace("\n", " ")
            emails.append(text.lower())
            labels.append(class_labels[author_dir])    

tokenizer = Tokenizer()
tokenizer.fit_on_texts(emails)
sequences = tokenizer.texts_to_sequences(emails)

word_index = tokenizer.word_index

data = pad_sequences(sequences, maxlen=max_sen_len)

labels = to_categorical(np.asarray(labels))
print labels.shape[0], labels.shape[1]

indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
nb_validation_samples = int(val_split * data.shape[0])

x_train = data[:-nb_validation_samples]
y_train = labels[:-nb_validation_samples]
x_val = data[-nb_validation_samples:]
y_val = labels[-nb_validation_samples:]

embeddings_index = {}
with open('glove.6B.100d.txt') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

embedding_matrix = np.random.random((len(word_index) + 1, emb_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
        
embedding_layer = Embedding(len(word_index) + 1,
                            emb_dim,
                            weights=[embedding_matrix],
                            input_length=max_sen_len,
                            trainable=True)

sequence_input = Input(shape=(max_sen_len,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)
l_lstm = Bidirectional(LSTM(100))(embedded_sequences)
dense_1 = Dense(128, activation="relu")(l_lstm)
drop_1 = Dropout(0.3)(dense_1)
preds = Dense(10, activation='softmax')(drop_1)
model = Model(sequence_input, preds)
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['acc'])

model.fit(x_train, y_train, validation_data=(x_val, y_val),
          epochs=30, batch_size=50)


dataProcessing.py

#!/usr/bin/python
import MySQLdb
import os
# Open database connection
db = MySQLdb.connect("localhost","root",<password>,"enron")

# prepare a cursor object using cursor() method
# abstraction meant for data set traversal
# one cursor per connection
cursor = db.cursor()

try:
   	# Execute the SQL command
	cursor.execute("""select E.firstName, E.lastName, M.body, M.mid from message as M, employeelist as E where (Email_id = sender or Email2 = sender or Email3 = sender or Email4 = sender) and M.folder LIKE '%Sent%' and eid in (10, 18, 92, 31, 137, 117, 20, 118, 65, 36, 112, 16);""")
	# Commit your changes in the database
   	db.commit()

	###########################################################
	###########################################################

	# Fetch a single row using fetchall() method.
	data_1 = cursor.fetchall()
	for record in data_1:
		#os.system('mkdir -p ' + 'clean_enron/' + record[1] + '_' + record[2])
		with open('./clean_enron/' + record[0] + '_' + record[1] + '/' + str(record[3]) + '.txt', 'w+') as f:
			f.write(record[2] + '\n')

	###########################################################
	###########################################################

	rc = cursor.rowcount
	print "\nRow count: ", rc

except:
   	# Rollback in case there is any error
   	db.rollback()
   	print ("Exception")

#close the cursor
cursor.close()

# close the connection
db.close()

feature_extraction_scripts :

adjperemail.py

#Command to run on terminal in the smai_proj folder:
#python3 adjperemail.py ./clean_enron/ > adjperemail.txt

import sys
import os
import nltk

classes = ['Benjamin_Rogers','Chris_Dorland','Drew_Fossum','Jeffrey_Shankman','Kevin_Presto','Kimberly_Watson','Lynn_Blair','Mark_Haedicke','Michelle_Cash', 'Phillip_Allen']
#classes = ['Benjamin_Rogers']

direc = sys.argv[1]
#direc = "./clean_enron/"

for c in classes:
	listing = os.listdir(direc+c)
	for filename in listing:
		f = open(direc+c+'/'+filename, 'r')
		text = f.read()
		tok = nltk.word_tokenize(text)
		postag = nltk.pos_tag(tok)
		count = 0
		for i in range(len(postag)):
			if postag[i][1] == "JJ" or postag[i][1] == "JJR" or postag[i][1] == "JJS":
				count = count + 1
		print(c+','+filename+','+str(count))
		f.close()

avgsenlenperemail.py

#Command to run on terminal in the smai_proj folder:
#python avgsentlenperemail.py ./clean_enron/ > avgsentlenperemail.txt

import sys
import os
from nltk.tokenize import sent_tokenize

classes = ['Benjamin_Rogers','Chris_Dorland','Drew_Fossum','Jeffrey_Shankman','Kevin_Presto','Kimberly_Watson','Lynn_Blair','Mark_Haedicke','Michelle_Cash', 'Phillip_Allen']
#classes = ['Benjamin_Rogers']
#print len(classes)

direc = sys.argv[1]
#direc = "./clean_enron/"

for c in classes:
	listing = os.listdir(direc+c)
	#numemail = len(listing)
	#count = 0
	for filename in listing:
		f = open(direc+c+'/'+filename, 'r')
		count = 0
		count1 = 0
		for line in f:
			count1 = count1+1
			line = line.strip('\n')
			count = count + len(line.split())
		#text = f.read()
		#list1 = sent_tokenize(text)
		#numsent = len(list1)
		#count = len(text.split())
		#count = 0
		#for item in list1:
		#	count = count + len(item.split())
		avgsentlen = count/float(count1)
		print c+','+filename+','+str(avgsentlen)
		f.close()
	#avgword = count/float(numemail)
	#print c,avgword

avgwordlenpermail.py

#Command to run on terminal in the smai_proj folder:
#python avgwordlenperemail.py ./clean_enron/ > avgwordlenperemail.txt

import sys
import os

classes = ['Benjamin_Rogers','Chris_Dorland','Drew_Fossum','Jeffrey_Shankman','Kevin_Presto','Kimberly_Watson','Lynn_Blair','Mark_Haedicke','Michelle_Cash', 'Phillip_Allen']
#classes = ['Benjamin_Rogers']
#print len(classes)

direc = sys.argv[1]
#direc = "./clean_enron/"

for c in classes:
	listing = os.listdir(direc+c)
	#numemail = len(listing)
	#count = 0
	for filename in listing:
		f = open(direc+c+'/'+filename, 'r')
		count = 0
		count1 = 0
		for line in f:
			line = line.strip('\n')
			list1 = line.split()
			count1 = count1 + len(list1)
			for item in list1:
				count = count + len(item)
			#count = count + len(line.split())
		#text = f.read()
		#count = len(text.split())
		avgwordlen = count/float(count1)
		print c+','+filename+','+str(avgwordlen)
		f.close()

charsperemail.py

#Command to run on terminal in the smai_proj folder:
#python charsperemail.py ./clean_enron/ > charsperemail.txt

import sys
import os
from nltk.tokenize import sent_tokenize

classes = ['Benjamin_Rogers','Chris_Dorland','Drew_Fossum','Jeffrey_Shankman','Kevin_Presto','Kimberly_Watson','Lynn_Blair','Mark_Haedicke','Michelle_Cash', 'Phillip_Allen']
#classes = ['Benjamin_Rogers']
#print len(classes)

direc = sys.argv[1]
#direc = "./clean_enron/"

for c in classes:
	listing = os.listdir(direc+c)
	#numemail = len(listing)
	#count = 0
	for filename in listing:
		f = open(direc+c+'/'+filename, 'r')
		count = 0
		for line in f:
			line = line.strip('\n')
			count = count + len(line)
		#text = f.read()
		#list1 = sent_tokenize(text)
		#numsent = len(list1)
		#count = len(text)
		#count = 0
		#for item in list1:
		#	count = count + len(item.split())
		#avgsentlen = count/float(numsent)
		print c+','+filename+','+str(count)
		f.close()
	#avgword = count/float(numemail)
	#print c,avgword


funcwordsperemail.py

#Command to run on terminal in the smai_proj folder:
#python3 funcwordsperemail.py ./clean_enron/ > funcwordsperemail.txt

import sys
import os
import nltk
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

classes = ['Benjamin_Rogers','Chris_Dorland','Drew_Fossum','Jeffrey_Shankman','Kevin_Presto','Kimberly_Watson','Lynn_Blair','Mark_Haedicke','Michelle_Cash', 'Phillip_Allen']
#classes = ['Benjamin_Rogers']
#print len(classes)

direc = sys.argv[1]
#direc = "./clean_enron/"

for c in classes:
	listing = os.listdir(direc+c)
	#numemail = len(listing)
	#count = 0
	for filename in listing:
		f = open(direc+c+'/'+filename, 'r')
		text = f.read()
		tok = nltk.word_tokenize(text)
		count = 0
		for item in tok:
			if item in stop_words:
				count = count + 1
		#postag = nltk.pos_tag(tok)
		print(c+','+filename+','+str(count))
		f.close()

perpronperemail.py

#Command to run on terminal in the smai_proj folder:
#python3 perpronperemail.py ./clean_enron/ > perpronperemail.txt

import sys
import os
import nltk

classes = ['Benjamin_Rogers','Chris_Dorland','Drew_Fossum','Jeffrey_Shankman','Kevin_Presto','Kimberly_Watson','Lynn_Blair','Mark_Haedicke','Michelle_Cash', 'Phillip_Allen']
#classes = ['Benjamin_Rogers']
#print len(classes)

direc = sys.argv[1]
#direc = "./clean_enron/"

for c in classes:
	listing = os.listdir(direc+c)
	#numemail = len(listing)
	#count = 0
	for filename in listing:
		f = open(direc+c+'/'+filename, 'r')
		text = f.read()
		tok = nltk.word_tokenize(text)
		postag = nltk.pos_tag(tok)
		count = 0
		for i in range(len(postag)):
			if postag[i][1] == "PRP":
				count = count + 1
		print(c+','+filename+','+str(count))
		f.close()

stylometric_vector.py

import sys
import os


# Create vectors of stylometric features from all the features
# Features must be in lexicographical ordering wrt (Author_name, file_name)

#####################################

file_ids = []

style_vectors = []

## file_names = 'adjperemail', 'funcwordsperemail', 'perpronperemail',
## 'avgsentlenperemail', 'avgwordlenperemail', 'charsperemail', 
## 'uniqbytotperemail', 'wordsperemail'

######################################
ctr = 0
l = []

with open('./extracted_features/adjperemail.txt', 'r') as f:
	for line in f:
		l = line.strip().split(',')
		style_vectors.append(l)
ctr += 3



with open('./extracted_features/funcwordsperemail.txt', 'r') as f:
	for line in f:
		l = line.split(',')
		for vec in style_vectors:
			if l[0] == vec[0] and l[1] == vec[1] :
				vec.append(l[2].strip())
ctr += 1
for vec in style_vectors:
	if len(vec) < ctr :
		vec.append(0.0)



with open('./extracted_features/perpronperemail.txt', 'r') as f:
	for line in f:
		l = line.split(',')
		for vec in style_vectors:
			if l[0] == vec[0] and l[1] == vec[1] :
				vec.append(l[2].strip())
ctr += 1
for vec in style_vectors:
	if len(vec) < ctr :
		vec.append(0.0)



with open('./extracted_features/avgsentlenperemail.txt', 'r') as f:
	for line in f:
		l = line.split(',')
		for vec in style_vectors:
			if l[0] == vec[0] and l[1] == vec[1] :
				vec.append(l[2].strip())
ctr += 1
for vec in style_vectors:
	if len(vec) < ctr :
		vec.append(0.0)


with open('./extracted_features/avgwordlenperemail.txt', 'r') as f:
	for line in f:
		l = line.split(',')
		for vec in style_vectors:
			if l[0] == vec[0] and l[1] == vec[1] :
				vec.append(l[2].strip())
ctr += 1
for vec in style_vectors:
	if len(vec) < ctr :
		vec.append(0.0)


with open('./extracted_features/charsperemail.txt', 'r') as f:
	for line in f:
		l = line.split(',')
		for vec in style_vectors:
			if l[0] == vec[0] and l[1] == vec[1] :
				vec.append(l[2].strip())
ctr += 1
for vec in style_vectors:
	if len(vec) < ctr :
		vec.append(0.0)


with open('./extracted_features/uniqbytotperemail.txt', 'r') as f:
	for line in f:
		l = line.split(',')
		for vec in style_vectors:
			if l[0] == vec[0] and l[1] == vec[1] :
				vec.append(l[2].strip())
ctr += 1
for vec in style_vectors:
	if len(vec) < ctr :
		vec.append(0.0)


with open('./extracted_features/wordsperemail.txt', 'r') as f:
	for line in f:
		l = line.split(',')
		for vec in style_vectors:
			if l[0] == vec[0] and l[1] == vec[1] :
				vec.append(l[2])
ctr += 1
for vec in style_vectors:
	if len(vec) < ctr :
		vec.append(0.0)
	if len(vec) != 10:
		print "Some vector is wrong"
		print vec


sv_file = open("stylometricVector.txt", "w+")

'''
element = ''

for vec in style_vectors:
	temp = ''
	for element in vec:
		temp += element.strip('\n') + ','
		temp.strip('\n')
	temp.strip()
	temp.strip(',')
	temp += '\n'
	sv_file.write(temp)

'''

for vec in style_vectors:
	temp = ','.join(vec)
	sv_file.write(temp)

sv_file.close()

uniqbytotperemail.py

#Command to run on terminal in the smai_proj folder:
#python uniqbytotperemail.py ./clean_enron/ > uniqbytotperemail.txt

import sys
import os

classes = ['Benjamin_Rogers','Chris_Dorland','Drew_Fossum','Jeffrey_Shankman','Kevin_Presto','Kimberly_Watson','Lynn_Blair','Mark_Haedicke','Michelle_Cash', 'Phillip_Allen']
#classes = ['Benjamin_Rogers']
#print len(classes)

direc = sys.argv[1]
#direc = "./clean_enron/"

for c in classes:
	listing = os.listdir(direc+c)
	#numemail = len(listing)
	#count = 0
	for filename in listing:
		f = open(direc+c+'/'+filename, 'r')
		count = 0
		list2 = []
		for line in f:
			line = line.strip('\n')
			list1 = line.split()
			for item in list1:
				list2.append(item)
		count1 = len(set(list2))
		count = len(list2)
		#text = f.read()
		#count = len(text.split())
		uniqbytot = count1/float(count)
		print c+','+filename+','+str(uniqbytot)
		f.close()

wordsperemail.py

#Command to run on terminal in the smai_proj folder:
#python wordsperemail.py ./clean_enron/ > wordsperemail.txt

import sys
import os

classes = ['Benjamin_Rogers','Chris_Dorland','Drew_Fossum','Jeffrey_Shankman','Kevin_Presto','Kimberly_Watson','Lynn_Blair','Mark_Haedicke','Michelle_Cash', 'Phillip_Allen']
#classes = ['Benjamin_Rogers']
#print len(classes)

direc = sys.argv[1]
#direc = "./clean_enron/"

for c in classes:
	listing = os.listdir(direc+c)
	#numemail = len(listing)
	#count = 0
	for filename in listing:
		f = open(direc+c+'/'+filename, 'r')
		count = 0
		for line in f:
			line = line.strip('\n')
			count = count + len(line.split())
		#text = f.read()
		#count = len(text.split())
		print c+','+filename+','+str(count)
		f.close()







